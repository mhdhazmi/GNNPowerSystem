\section{Introduction}
\label{sec:intro}

% Paragraph 1: Motivation and Operational Context
Modern power grid operations demand rapid decision-making capabilities that traditional computational methods struggle to provide. Optimal power flow ({OPF}) solvers---used to determine economically efficient generator dispatch while respecting physical and operational constraints---can require minutes to solve on utility-scale networks with thousands of buses~\cite{huang2022deepopfv}, rendering them impractical for real-time control and contingency analysis. Cascading failure risk assessment presents an even greater challenge: comprehensive {N-k} security analysis requires evaluating thousands or millions of contingency scenarios, each necessitating a full power flow calculation~\cite{du2019achieving}. These computational bottlenecks have motivated a shift toward machine learning surrogates that approximate complex power system computations at dramatically reduced cost. Recent supervised learning approaches have demonstrated remarkable speedups---ranging from 100$\times$ to 10,000$\times$ faster than conventional solvers~\cite{pan2021deepopf,huang2022deepopfv}---while maintaining solution quality within 0.2\% of optimality. However, these methods rely fundamentally on labeled training data: typical implementations require 5,000 to 60,000 labeled samples per network~\cite{pan2021deepopf,mohammadi2024surrogate}, where each label is itself the output of the computationally expensive solver the surrogate aims to replace. This chicken-and-egg problem---needing expensive simulations to train models meant to avoid expensive simulations---severely limits the practical deployment of learning-based power system analysis, particularly for networks with limited historical data or frequently changing topologies.

% Paragraph 2: The Promise of Self-Supervised Learning
Self-supervised learning ({SSL}) offers a compelling solution to the labeled data bottleneck by leveraging abundant unlabeled operational data. In the broader machine learning community, graph {SSL} methods have demonstrated substantial performance gains in low-label regimes: {GraphMAE2} achieves a 5.46 percentage point accuracy improvement using only 1\% labeled data on molecular property prediction benchmarks~\cite{hou2023graphmae2}, while contrastive learning frameworks enable effective transfer across diverse graph domains~\cite{qiu2020gcc}. These successes suggest that self-supervised pretraining could similarly benefit power system applications, where operational measurements (bus voltages, line flows, injection patterns) are continuously recorded but corresponding ``labels'' (optimal solutions, failure classifications) are costly to obtain. However, a critical gap exists: among the extensive body of graph {SSL} research spanning molecular graphs~\cite{liu2022graphmvp}, social networks~\cite{qiu2020gcc}, and traffic systems~\cite{ji2023stssl}, applications to power grids remain remarkably scarce. Our comprehensive literature review identified only five to six papers applying {SSL} to power systems~\cite{donon2020graph,park2023primal}, and critically, \emph{none} of these methods incorporate physics-informed pretext tasks that leverage the fundamental electrical relationships governing grid behavior. This represents a significant missed opportunity: power systems are governed by well-understood physical laws---Kirchhoff's current and voltage laws, power flow equations, admittance relationships---that could serve as powerful self-supervisory signals for representation learning.

% Paragraph 3: Physics-Guided Graph Neural Networks
The graph structure of electrical networks naturally aligns with graph neural network ({GNN}) architectures, where message passing along edges can mirror the physical propagation of power flows along transmission lines~\cite{kipf2017semi}. Recent work has begun embedding power system physics into {GNN} designs: impedance-weighted aggregation schemes~\cite{wu2025meta}, complex-valued representations preserving phase relationships~\cite{wu2024complex}, and hard Kirchhoff's law constraints enforced through architectural projections~\cite{dogoulis2025kclnet}. These physics-informed {GNN}s demonstrate improved accuracy and generalization compared to topology-agnostic baselines, particularly for out-of-distribution scenarios and {N-1} contingencies~\cite{thangamuthu2022unravelling}. However, existing physics-guided approaches remain tethered to supervised learning paradigms, requiring labeled power flow solutions or optimal dispatch setpoints for training. \emph{No prior work has combined physics-guided {GNN} architectures with self-supervised pretraining}, leaving unexplored the question of whether physics-informed message passing can enable effective representation learning from unlabeled data alone.

% Paragraph 4: Research Contributions
This paper introduces a physics-guided self-supervised learning framework for power grid analysis that addresses these gaps. We make the following contributions:

\begin{itemize}[leftmargin=*,itemsep=2pt]
    \item \textbf{Physics-Guided Graph Neural Network Architecture:} We design a message-passing encoder that incorporates admittance-weighted aggregation, where line conductance and susceptance values directly control information flow between connected buses, embedding the structure of the power system admittance matrix into the neural network computation graph.
    
    \item \textbf{Self-Supervised Pretraining Objective:} We develop grid-specific pretext tasks---masked injection reconstruction (predicting active and reactive power injections at randomly masked buses) and masked parameter reconstruction (predicting line impedance parameters at randomly masked edges)---that enable representation learning from unlabeled grid operational data without requiring solutions from conventional solvers. Critically, we ensure no label leakage by pretraining exclusively on the training partition, with validation and test sets never exposed during self-supervised learning.
    
    \item \textbf{Multi-Task Transfer Learning Evaluation:} We demonstrate that representations learned via physics-guided {SSL} transfer effectively to three downstream tasks: (1)~power flow prediction (bus voltage magnitudes), (2)~line flow prediction (active and reactive power flows on transmission lines), and (3)~cascading failure classification (graph-level binary prediction of cascade occurrence). On the {PowerGraph} benchmark~\cite{varbella2024powergraph}, our approach achieves 29.1\% mean absolute error reduction for power flow, 26.4\% reduction for line flow, and 6.8\% F1-score improvement for cascade prediction---all measured at 10\% labeled data availability relative to scratch training baselines.
    
    \item \textbf{Scalability and Variance Reduction:} On the larger {IEEE} 118-bus system under severe class imbalance conditions (5\% cascade rate), self-supervised pretraining not only improves mean performance ($\Delta$F1 = +0.61) but dramatically reduces training instability: scratch training exhibits $\pm$0.243 F1 variance across random seeds, while {SSL}-pretrained models achieve $\pm$0.051 variance---a stabilization effect critical for reliable deployment.
    
    \item \textbf{Explainability Validation:} Using ground-truth edge importance masks from the {PowerGraph} benchmark, we quantitatively evaluate explanation fidelity via Integrated Gradients, achieving 0.93 {AUC-ROC} compared to 0.72 for heuristic baselines. This addresses a recognized gap in cascading failure prediction, where current explainable {AI} methods have been reported to perform ``suboptimally''~\cite{varbella2024powergraph}.
    
    \item \textbf{Robustness Under Distribution Shift:} We evaluate model behavior under load stress conditions (1.0$\times$ to 1.3$\times$ nominal loading), demonstrating that {SSL}-pretrained representations exhibit superior robustness, maintaining 22\% higher performance advantage at 1.3$\times$ load compared to scratch training.
\end{itemize}

% Paragraph 5: Paper Organization
The remainder of this paper is organized as follows. Section~\ref{sec:related} reviews related work in power system machine learning, graph neural networks, self-supervised learning, physics-informed methods, and cascading failure prediction. Section~\ref{sec:problem} formulates the graph representation of power grids and defines the three downstream tasks. Section~\ref{sec:method} details our physics-guided encoder architecture and self-supervised pretraining objective. Section~\ref{sec:experiments} describes the experimental setup, including datasets, baselines, and training protocols. Section~\ref{sec:results} presents comprehensive results across all tasks and grid scales, including ablation studies and robustness analysis. Section~\ref{sec:discussion} discusses implications for operational deployment, limitations, and future directions. Section~\ref{sec:conclusion} concludes.
