\section{Conclusion}
\label{sec:conclusion}

We presented a physics-guided self-supervised learning framework for graph neural networks that addresses labeled data scarcity in power grid analysis. By embedding electrically-parameterized message passing into the encoder architecture---where edge weights are learned from line admittance features---and developing grid-specific pretext tasks (masked injection and parameter reconstruction), our approach learns representations from unlabeled operational data that transfer effectively to downstream tasks. Multi-seed validation across three prediction tasks on two grid scales confirms consistent improvements in low-label regimes: at 10\% labeled data, self-supervised pretraining achieves 29.1\% power flow error reduction, 26.4\% line flow error reduction, and 6.8\% F1-score improvement for cascading failure prediction on the {IEEE} 24-bus system. On the {IEEE} 118-bus system, pretraining dramatically stabilizes cascade prediction training, reducing F1 variance from $\pm$0.243 (scratch) to $\pm$0.051 ({SSL})---a critical reliability improvement for deployment in safety-critical infrastructure where performance cannot depend on fortunate random initialization.

Beyond sample efficiency, our framework achieves 0.93 {AUC-ROC} explainability fidelity via Integrated Gradients, correctly identifying critical transmission lines whose failures drive cascades. Robustness evaluation under load stress (1.0--1.3$\times$ nominal) with 5-seed validation demonstrates that self-supervised representations generalize more effectively to out-of-distribution conditions (+3.6\% relative advantage at 1.3$\times$ load). The consistent pattern across all tasks---largest gains when labeled data is most scarce---validates our hypothesis that physics-guided self-supervised learning captures fundamental grid structure and electrical relationships that reduce dependence on task-specific supervision. However, practitioners should note that in data-abundant regimes ($>$50\% labeled data), SSL pretraining provides marginal benefits and can slightly impair regression performance on larger grids; scratch training may be preferable when labeled data is plentiful.

Future work should extend evaluation to real utility datasets with operational measurements, dynamic topology changes, and diverse distribution shifts (measurement noise, renewable variability, seasonal patterns). Investigating mechanistic explainability beyond edge importance ranking, developing few-shot transfer learning across multiple grids, and exploring continual learning as grids evolve would further advance practical deployment of machine learning for power system operations. Our framework demonstrates that self-supervised learning, when designed with domain physics in mind, provides a viable path toward sample-efficient, reliable, and interpretable machine learning for critical infrastructure.
