\section{Discussion}
\label{sec:discussion}

\subsection{Why Self-Supervised Learning Improves Low-Label Performance}
\label{subsec:why_ssl_works}

Our results demonstrate consistent improvements from self-supervised pretraining across all tasks, with the most substantial gains (6.8--29.1\%) occurring when labeled data is severely limited (10\% training fraction). This effectiveness stems from three complementary mechanisms.

\textbf{Physics-meaningful pretext tasks:} Unlike generic graph {SSL} methods that mask arbitrary node features~\cite{hou2022graphmae,thakoor2022bgrl}, our masked injection reconstruction objective directly targets power system quantities governed by physical laws. To reconstruct a masked bus's active power injection $P_i$, the encoder must implicitly learn Kirchhoff's Current Law: power balance requires that injections equal the net sum of flows on connected lines. Similarly, masked edge parameter reconstruction (reactance, thermal ratings) forces the model to understand how line impedances relate to voltage drops and power transfer capabilities via Ohm's Law for {AC} circuits. This contrasts with supervised learning, which directly observes voltage or flow labels, potentially enabling shortcuts that bypass physical understanding. Self-supervised reconstruction cannot exploit such shortcuts---the model must learn the underlying physics to succeed at the pretext task.

\textbf{Representation initialization and loss landscape geometry:} Neural network training dynamics are heavily influenced by initialization~\cite{glorot2010understanding}. Random initialization places parameters in arbitrary regions of the loss landscape, requiring supervised gradient descent to navigate from scratch. In low-label regimes, sparse supervision provides weak gradients that may converge to poor local minima or exhibit high variance across random seeds (as observed with {IEEE} 118-bus scratch training at 10\% labels: F1 variance $\pm$0.243). Self-supervised pretraining moves encoder parameters into favorable loss landscape regions where the model has already learned to represent grid topology, electrical coupling strengths, and power balance relationships. Subsequent fine-tuning on task-specific labels then requires only modest adjustments rather than learning representations from scratch, enabling more reliable and sample-efficient convergence.

\textbf{Shared structural patterns across tasks:} Cascade prediction, power flow approximation, and line flow estimation all depend fundamentally on understanding how power transfers through grid topology. A bus's voltage magnitude is determined by its injection and connected lines' impedances; a line's power flow depends on connected buses' voltages; a cascade propagates along paths of overloaded lines connecting vulnerable buses. Our {SSL} pretraining captures these shared structural patterns by forcing the encoder to predict local electrical quantities from neighborhood context, yielding representations that transfer effectively across multiple downstream tasks. This explains why {SSL} benefits persist even at 100\% labels, albeit diminished: the pretrained encoder has learned complementary features beyond task-specific supervision.

\subsection{Operational Implications and Deployment Considerations}
\label{subsec:operational}

Deploying machine learning models for real-time grid operations requires careful consideration of what information is observable without computationally expensive simulations. Our approach is designed for practical deployment scenarios where only measurements and network parameters are available, not solutions from conventional solvers.

\textbf{Observability assumptions:} At inference time, our models assume access to: (1)~Bus-level measurements of active and reactive power injections ($P_{\text{net}}$, $S_{\text{net}}$) from supervisory control and data acquisition ({SCADA}) systems or phasor measurement units ({PMU}s), (2)~Network topology and line parameters (conductance, susceptance, reactance, thermal ratings) from grid databases, and (3)~For cascade prediction and line flow tasks, current bus voltage magnitudes from state estimation. Critically, we do \emph{not} assume access to power flow solutions, optimal dispatch decisions, or cascade simulation outputs at inference time---these are the expensive computations our models replace. For power flow prediction specifically, voltage magnitudes are excluded from input features since they constitute the prediction target.

\textbf{No-oracle deployment:} Unlike some prior work that assumes oracle knowledge of future grid states or failure outcomes~\cite{donon2020neural}, our models make predictions based solely on pre-event measurements. For cascade prediction, we observe the grid state before a contingency occurs and predict whether a cascade will result, without knowing which specific lines will fail or having access to intermediate cascade propagation states. This no-oracle constraint is essential for practical deployment: operators need to assess cascade risk \emph{before} taking preventive actions, not after the cascade has already begun.

\textbf{Computational efficiency:} Self-supervised pretraining is performed offline once per grid, requiring approximately 30 minutes on a single {GPU} for {IEEE} 24-bus and 2 hours for {IEEE} 118-bus. Once pretrained, the encoder can be fine-tuned for multiple downstream tasks without repeating pretraining. Inference is extremely fast: cascade prediction for a single grid state requires $<$10 milliseconds on {CPU}, enabling real-time contingency screening. Power flow and line flow prediction similarly achieve sub-second inference times, orders of magnitude faster than iterative numerical solvers that require seconds to minutes per solve.

\subsection{Scalability Findings: IEEE 118-Bus System}
\label{subsec:scalability_discussion}

The {IEEE} 118-bus results reveal a nuanced but operationally critical finding: self-supervised pretraining is most valuable when labeled data is extremely scarce relative to problem difficulty. At 10\% labeled data on the large-scale {IEEE} 118-bus system, scratch training exhibits catastrophic instability with F1 variance $\pm$0.243---some random seeds converge to reasonable performance (F1 $\approx$ 0.60), while others fail completely (F1 $\approx$ 0.05). This variance indicates that scratch training in severe low-label regimes is unreliable: deployment success depends on fortunate random initialization, an unacceptable risk for safety-critical infrastructure.

Self-supervised pretraining eliminates this instability: all five random seeds converge to F1 = 0.87 $\pm$ 0.051, providing consistent performance regardless of initialization. This stability advantage diminishes at higher label fractions---by 20\% labeled data, both scratch and {SSL} methods achieve reliable convergence (F1 $>$ 0.83) with low variance. The practical implication is clear: when labeled cascade data is expensive or impossible to obtain (e.g., rare blackout events with limited historical records), {SSL} pretraining provides the reliability necessary for operational deployment. When labeled data is abundant ($>$20\% of available samples), both approaches work well, and the choice between them becomes less critical.

\textbf{Class imbalance interaction:} The {IEEE} 118-bus cascade dataset exhibits severe class imbalance (approximately 5\% positive class rate), compounding the low-label challenge. With only 10\% labeled data, scratch training observes fewer than 500 positive (cascade) examples across 91,875 training samples---insufficient for learning rare failure patterns. Self-supervised pretraining mitigates this by learning grid structure from \emph{all} unlabeled training data, not just the small labeled subset, providing a representation foundation that requires fewer labeled cascade examples to achieve reliable classification.

\subsection{Limitations and Future Directions}
\label{subsec:limitations}

While our results demonstrate clear benefits of physics-guided self-supervised learning, several limitations warrant discussion and suggest directions for future research.

\textbf{Single benchmark evaluation:} All experiments use the {PowerGraph} benchmark~\cite{varbella2024powergraph} on simulated {IEEE} test systems. Validation on real utility datasets with operational measurements, measurement noise, missing data, and dynamic topology changes is essential before deployment. Real grids exhibit complexities not captured in simulation: communication delays, bad data from faulty sensors, topology errors in network models, and time-varying renewable generation. Transfer learning experiments demonstrating that models pretrained on simulated data can fine-tune effectively on small real-world labeled datasets would strengthen deployment confidence.

\textbf{Static topology assumption:} Our current framework assumes fixed grid topology during training and inference. Real power systems undergo frequent topology changes due to maintenance outages, equipment failures, and switching operations for loss minimization. Extending our approach to handle dynamic topology---for example, via graph structure learning~\cite{franceschi2019learning} or topology-conditional embeddings---would improve practical applicability. Additionally, investigating how well models generalize to unseen topologies (e.g., training on {IEEE} 24-bus, testing on {IEEE} 30-bus) would clarify the transferability of learned physics principles across grid structures.

\textbf{Limited out-of-distribution evaluation:} We evaluated robustness only under load scaling (1.0--1.3$\times$ nominal injections). Other critical distribution shifts remain unexplored: (1)~Measurement noise and missing data, (2)~Seasonal and diurnal load pattern variations, (3)~High renewable penetration with intermittent generation, (4)~Topology perturbations (line outages), and (5)~Extreme weather events causing correlated failures. A comprehensive robustness evaluation across multiple {OOD} dimensions is necessary to establish reliability bounds for operational deployment. Additionally, our preliminary load scaling results used a single random seed and should be validated with multi-seed experiments for statistical significance.

\textbf{Computational cost of pretraining:} While inference is fast, self-supervised pretraining requires 30 minutes to 2 hours of {GPU} time depending on grid size. For utilities with hundreds of distinct network models (e.g., different seasonal configurations, multiple voltage levels), pretraining all models could incur significant computational cost. Investigating few-shot transfer learning---pretraining once on a large representative grid, then fine-tuning with minimal data on similar grids---could amortize pretraining costs across multiple deployment scenarios.

\textbf{Explainability depth:} Our Integrated Gradients evaluation demonstrates that the model correctly ranks critical edges (0.93 {AUC-ROC}), but does not provide \emph{mechanistic} explanations of \emph{why} specific lines are vulnerable. Causal discovery methods~\cite{pearl2009causality} or attention mechanism interpretation~\cite{vaswani2017attention} could yield deeper insights into learned failure propagation patterns, enabling operators to understand not just \emph{which} lines matter but \emph{how} cascades propagate through grid topology.
