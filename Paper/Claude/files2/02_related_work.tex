\section{Related Work}
\label{sec:related}

% Paragraph 1: Power System Surrogate Models and Data Requirements
Traditional power flow and optimal power flow computations rely on iterative numerical methods such as Newton-Raphson and interior-point algorithms, which can require seconds to minutes per solve on large-scale grids~\cite{pan2021deepopf}. To enable near-real-time decision support, researchers have developed machine learning surrogates that approximate these complex mappings from grid conditions (load demands, generation capacities, network topology) to power flow solutions or optimal dispatch setpoints. Fully-connected deep neural networks have demonstrated impressive results: Pan et al.'s {DeepOPF} framework achieves feasible {OPF} solutions with less than 0.2\% optimality loss and up to 100$\times$ speedup over conventional solvers on benchmark {IEEE} test systems~\cite{pan2021deepopf}, while Huang et al.'s {DeepOPF-V} extends this approach to {AC-OPF} with reported speedups exceeding 10,000$\times$ on 2,000-bus networks~\cite{huang2022deepopfv}. Graph neural networks have emerged as a particularly promising architecture due to their ability to exploit grid topology and achieve greater scalability~\cite{lin2024powerflownet,liu2023topology}. For instance, {PowerFlowNet} demonstrated successful scaling to a 6,470-bus French transmission network with voltage magnitude prediction errors below 0.001 per-unit~\cite{lin2024powerflownet}, while heterogeneous message-passing neural networks maintain constant parameter counts across grid sizes ranging from 14 to 2,000+ buses~\cite{arowolo2025heterogeneous}. However, these supervised approaches face a critical limitation: they require extensive labeled training data. Typical implementations demand 5,000--60,000 {OPF} solutions per network~\cite{pan2021deepopf,mohammadi2024surrogate}, and generating this labeled data via conventional solvers is computationally expensive---precisely the bottleneck these methods aim to circumvent. This data scarcity challenge motivates unsupervised and self-supervised learning approaches that can leverage abundant unlabeled operational data.

% Paragraph 2: Graph Neural Networks for Power Grid Topology
The natural graph structure of electrical networks---with buses as nodes and transmission lines as edges---makes graph neural networks an ideal architecture for power system analysis. {GNN}s encode topological relationships through message passing, where each node aggregates information from its neighbors according to the grid's physical connectivity~\cite{kipf2017semi}. This inductive bias enables {GNN}s to handle topology changes (such as {N-1} contingencies) without retraining, a critical advantage over fully-connected networks that treat grid states as fixed-dimensional vectors~\cite{donti2021dc3}. Empirical studies demonstrate that {GNN}-based approaches achieve substantially lower prediction errors than traditional neural networks: for example, an electrical-model-guided {GNN} for distribution system state estimation attained an order-of-magnitude lower error than standard feedforward networks, even with missing sensor measurements~\cite{lin2022elegnn}. Recent work has begun incorporating power system physics more deeply into {GNN} architectures. {Meta-PIGACN} integrates impedance-weighted edge aggregation, where line admittance values directly control message-passing strength~\cite{wu2025meta}, while complex-valued spatial-temporal {GCN}s represent voltage phasors and impedance in their native complex form to preserve phase relationships inherent to {AC} power flow~\cite{wu2024complex}. {KCLNet} enforces Kirchhoff's Current Law as hard architectural constraints via differentiable hyperplane projections, guaranteeing zero {KCL} violations by construction~\cite{dogoulis2025kclnet}, and {PINCO} achieves zero inequality constraint violations through physics-informed hard constraints in an unsupervised learning framework~\cite{pinco2025}. The {PowerGraph} benchmark~\cite{varbella2024powergraph} provides standardized evaluation across {GCN}, {GAT}, {GraphSAGE}, and {Graph Transformer} architectures, establishing that topology-aware message passing consistently outperforms topology-agnostic baselines on both node-level (power flow, voltage estimation) and graph-level (cascading failure prediction) tasks.

% Paragraph 3: Self-Supervised Learning for Graphs
Self-supervised learning has emerged as a powerful paradigm for learning graph representations without labeled data, with two dominant approaches: contrastive learning and generative masked reconstruction. Contrastive methods, exemplified by {Deep Graph Infomax}~\cite{velickovic2019dgi}, {InfoGraph}~\cite{sun2020infograph}, and {GraphCL}~\cite{you2020graphcl}, maximize agreement between different augmented views of graphs through node dropping, edge perturbation, or subgraph sampling. More recent frameworks have reduced the complexity of these approaches: {BGRL} eliminates negative sampling through bootstrapped representation learning~\cite{thakoor2022bgrl}, achieving 2--10$\times$ memory reduction while matching state-of-the-art performance, and {SimGRACE} dispenses with manual graph augmentation entirely by perturbing encoder parameters to generate contrastive views~\cite{xia2022simgrace}. In parallel, generative approaches have gained traction: {GraphMAE} employs masked feature reconstruction with a scaled cosine error loss and dedicated decoder architecture~\cite{hou2022graphmae}, demonstrating that carefully designed autoencoders can match or exceed contrastive methods (84.2\% accuracy on Cora versus 82.7\% for {BGRL}). {GraphMAE2} extends this with multi-view random re-masking and latent representation prediction~\cite{hou2023graphmae2}, scaling to graphs with over 100 million nodes. These graph {SSL} methods show substantial benefits in low-label regimes: {GraphMAE2} achieves a 5.46 percentage point accuracy improvement with only 1\% labeled data on large-scale molecular property prediction benchmarks~\cite{hou2023graphmae2}, while {GCC}'s cross-domain pretraining on diverse network types enables effective transfer to new graph tasks with minimal fine-tuning~\cite{qiu2020gcc}. Applications to physical systems have proven successful in molecular graphs~\cite{liu2022graphmvp} and traffic networks~\cite{ji2023stssl}, where graph structure naturally encodes physical relationships. However, a significant research gap exists for power grid applications: while {SafePowerGraph}~\cite{ringsquandl2024safepowergraph} and recent work by Zhu et al.~\cite{zhu2025physics} introduced hybrid supervised-{SSL} approaches for power systems, no pure graph {SSL} method has been designed specifically for electrical networks with physics-informed constraints such as power flow equations, Kirchhoff's laws, or voltage-angle relationships.

% Paragraph 4: Physics-Informed Machine Learning
Physics-informed neural networks embed domain knowledge---governing equations, conservation laws, or known constraints---into machine learning models to improve generalization and sample efficiency. The canonical {PINN} framework~\cite{raissi2019physics} encodes partial differential equations as soft regularization terms in the loss function, minimizing both data mismatch and {PDE} residuals at collocation points. This approach has proven effective for solving forward and inverse problems in fluid dynamics, solid mechanics, and heat transfer~\cite{karniadakis2021physics}. Alternative strategies include hard constraint enforcement, where physical laws are satisfied by construction through architectural design: Beucler et al. demonstrated that architecture-constrained networks can achieve energy and mass conservation to machine precision without loss penalties~\cite{beucler2021enforcing}, while Hamiltonian Neural Networks embed symplectic structure to guarantee exact energy conservation in dynamical systems~\cite{greydanus2019hamiltonian}. In power systems specifically, physics-informed approaches have addressed swing equation dynamics for transient stability~\cite{misyris2020physics}, state estimation with admittance matrix constraints~\cite{zamzam2020physics}, and power flow prediction with Kirchhoff's law regularization~\cite{hu2021physics}. These methods demonstrate compelling benefits: physics constraints act as inductive biases that limit the hypothesis space and improve out-of-distribution generalization, with physics-informed {GNN}s showing zero-shot generalizability to systems an order of magnitude larger than training configurations~\cite{thangamuthu2022unravelling}. However, a critical gap remains: the vast majority of {PINN} research targets continuous {PDE}-governed domains where automatic differentiation naturally computes spatial and temporal derivatives. Discrete, graph-structured infrastructure networks like power grids present fundamentally different challenges---physical laws ({KCL}, {KVL}, power balance) operate directly on graph edges and nodes rather than as discretized continuous fields, and topological changes require handling discrete switching dynamics. While {GraPhyR}~\cite{authier2024physics} demonstrated physics-informed {GNN}s with gated message passing for power system reconfiguration, graph-based power system surrogates that combine self-supervised pretraining with physics-guided architectures remain unexplored.

% Paragraph 5: Cascading Failure Prediction and Explainability
Cascading failures---sequences of component outages that can escalate to widespread blackouts---represent a critical threat to power grid resilience, as evidenced by major disruptions including the 2003 Northeast blackout and the 2021 Texas winter storm~\cite{USCanada2004}. Traditional simulation models such as {OPA}~\cite{carreras2002critical}, {DCSIMSEP}~\cite{eppstein2012random}, and the Manchester model~\cite{nedic2006criticality} capture cascading dynamics through iterative power flow calculations coupled with protection system logic, revealing self-organized criticality in grid behavior. However, these physics-based simulators face computational limitations: {DC} cascade models provide two orders of magnitude speedup over {AC} models but sacrifice accuracy by ignoring voltage collapse mechanisms~\cite{cetinay2018analyzing}, while {AC} models are approximately 7$\times$ more computationally expensive and suffer convergence issues under high stress conditions. Machine learning approaches have achieved dramatic acceleration: deep convolutional neural networks enable 100$\times$ faster {N-1} contingency screening~\cite{du2019achieving}, while graph neural networks leverage network topology to predict cascade outcomes with over 96\% accuracy~\cite{varbella2023geometric}. {GNN}-based methods demonstrate transfer learning capability across different grid topologies and operating conditions, addressing a key limitation of classical simulation approaches. However, explainability remains a significant gap. Post-mortem analyses of major blackouts still rely on manual timeline reconstruction and root cause analysis~\cite{USCanada2004}, and recent benchmarking reveals that current explainable {AI} methods perform ``suboptimally'' on cascade explanation tasks~\cite{varbella2024powergraph}. The {PowerGraph} benchmark explicitly identifies this gap, noting that ``given the crucial role of explainability for power grid operators, this underscores the ongoing need for dedicated research and development in this field''~\cite{varbella2024powergraph}. While {XAI} techniques including {SHAP}, {LIME}, and attention mechanisms have been successfully applied to other power system tasks such as frequency stability prediction, cascading failure prediction lacks robust explainability frameworks that can identify critical transmission pathways and quantify failure propagation mechanisms in a way that operators can trust and act upon.

% Paragraph 6: Positioning This Work
This work addresses the identified gaps by uniquely combining physics-guided graph neural network architectures with self-supervised pretraining for power grid analysis. Our physics-guided encoder embeds admittance-weighted message passing directly into the network architecture---drawing on the success of {Meta-PIGACN}~\cite{wu2025meta} and {KCLNet}~\cite{dogoulis2025kclnet} but extending to a self-supervised learning paradigm. Unlike existing {SSL} approaches for graphs that ignore domain physics~\cite{hou2022graphmae,thakoor2022bgrl}, we design grid-specific pretext tasks (masked injection reconstruction, masked parameter reconstruction) that leverage power system structure without requiring labeled solutions from conventional solvers. This addresses the data scarcity challenge identified in supervised power flow learning~\cite{pan2021deepopf,mohammadi2024surrogate}, enabling effective representation learning from the abundant unlabeled operational data available in modern power grids. We demonstrate transfer learning across multiple tasks (power flow, line flow prediction, cascading failure classification) and grid scales ({IEEE} 24-bus and 118-bus systems), validating the generalizability of learned representations. Finally, we provide quantitative explainability evaluation using ground-truth explanation masks from the {PowerGraph} benchmark~\cite{varbella2024powergraph}, achieving 0.93 {AUC-ROC} fidelity for edge importance attribution via Integrated Gradients---addressing the explainability gap in cascading failure prediction and providing operators with interpretable risk assessments.
