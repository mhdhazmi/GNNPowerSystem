\section{Conclusion}
\label{sec:conclusion}

We presented a physics-guided self-supervised learning framework for graph neural networks that addresses labeled data scarcity in power grid analysis. By embedding admittance-weighted message passing into the encoder architecture and developing grid-specific pretext tasks (masked injection and parameter reconstruction), our approach learns representations from unlabeled operational data that transfer effectively to downstream tasks. Multi-seed validation across three prediction tasks on two grid scales confirms consistent improvements in low-label regimes: at 10\% labeled data, self-supervised pretraining achieves 29.1\% power flow error reduction, 26.4\% line flow error reduction, and 6.8\% F1-score improvement for cascading failure prediction on the {IEEE} 24-bus system. On the {IEEE} 118-bus system, pretraining dramatically stabilizes cascade prediction training, reducing F1 variance from $\pm$0.243 (scratch) to $\pm$0.051 ({SSL})---a critical reliability improvement for deployment in safety-critical infrastructure where performance cannot depend on fortunate random initialization.

Beyond sample efficiency, our framework achieves 0.93 {AUC-ROC} explainability fidelity via Integrated Gradients, correctly identifying critical transmission lines whose failures drive cascades. Preliminary robustness evaluation under load stress (1.0--1.3$\times$ nominal) suggests that self-supervised representations generalize more effectively to out-of-distribution conditions (+22\% advantage at 1.3$\times$ load), though multi-seed validation of this finding is needed. The consistent pattern across all tasks---largest gains when labeled data is most scarce, diminishing but persistent benefits even at full labels---validates our hypothesis that physics-guided self-supervised learning captures fundamental grid structure and electrical relationships that reduce dependence on task-specific supervision.

Future work should extend evaluation to real utility datasets with operational measurements, dynamic topology changes, and diverse distribution shifts (measurement noise, renewable variability, seasonal patterns). Investigating mechanistic explainability beyond edge importance ranking, developing few-shot transfer learning across multiple grids, and exploring continual learning as grids evolve would further advance practical deployment of machine learning for power system operations. Our framework demonstrates that self-supervised learning, when designed with domain physics in mind, provides a viable path toward sample-efficient, reliable, and interpretable machine learning for critical infrastructure.
