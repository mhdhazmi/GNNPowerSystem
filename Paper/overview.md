
---

## 5) Tech Stack

- Python ≥ 3.10
- PyTorch (2.x)
- PyTorch Geometric (PyG)
- numpy / scipy
- matplotlib
- experiment tracking: TensorBoard (default) or W&B (optional)
- optional:
  - pandapower
  - Optuna (hyperparameter search)
  - umap-learn (embedding viz)

---

## 6) Onboarding Checklist (Day 1)

1) **Set up environment**
   - Create a virtual env using uv
   - Install dependencies

2) **Get the dataset**
   - Download PowerGraph into `data/raw/`
   - Do not commit raw data to git

3) **Run the smoke test**
   - Confirms loader + model forward pass works
   - Command:
     - `python scripts/smoke_test.py`

4) **Run a tiny training run**
   - PF baseline on a very small subset
   - Command:
     - `python scripts/train_pf.py --config configs/pf_debug.yaml`

**Definition of “onboarded”:**
- You can run smoke_test successfully
- You can train a PF model for a few epochs and produce a log directory

---

## 7) Project Work Packages (What We’re Implementing)

### WP1 — Data ingestion & splits (PowerGraph → PyG)
- Build a robust loader producing PyG `Data` objects with:
  - node features (`x`)
  - edges (`edge_index`)
  - edge features (`edge_attr`)
  - PF targets / OPF targets / cascade labels
  - cascade explanation masks (if present)
- Create stable **train/val/test splits**, saved to `data/splits/`.

**Correctness checks**
- No NaNs/Infs
- Node/edge counts match expected
- Split files are deterministic and reused (no ad-hoc random splits)

---

### WP2 — PF baseline (single-task)
- Implement GNN baseline (GCN/SAGE/GAT) to predict PF state.
- Treat angles correctly (e.g., predict sin/cos).

**Correctness checks**
- Overfit test on tiny subset (loss drops sharply)
- Beats trivial baseline (mean predictor)

---

### WP3 — Physics consistency metric (and optional regularization)
- Implement a physics residual metric (e.g., mismatch proxy).
- Track as a metric; optionally add a regularization term.

**Correctness checks**
- Ground-truth labels should have far smaller residual than random predictions
- Physics penalty should reduce residual without destroying accuracy

---

### WP4 — OPF head + PF/OPF multitask
- Add OPF prediction head(s).
- Joint training with shared encoder + task-specific heads.

**Correctness checks**
- PF does not collapse after adding OPF
- Masked targets (if any) are handled correctly

---

### WP5 — Self-supervised pretraining (SSL)
- Implement grid-specific masking objective(s):
  - masked injection reconstruction OR masked edge-feature reconstruction
- Pretrain encoder, then fine-tune on PF and PF+OPF (low-label curves).

**Correctness checks**
- SSL loss decreases cleanly
- Frozen-encoder linear probe > random-init baseline (esp. low-label)

---

### WP6 — Cascading failure prediction + explanation fidelity
- Train cascade classifier/regressor head.
- Evaluate explanation fidelity against ground-truth masks (AUC, Precision@K).
- Test transfer: scratch vs PF/OPF-pretrained vs SSL-pretrained encoders.

**Correctness checks**
- Random explanation scores ≈ chance AUC
- Transfer improves either accuracy, robustness, or explanation fidelity

---

## 8) Reproducibility Rules (Non-Negotiable)

1) **All experiments must be runnable from scripts**, not only notebooks.
2) Every run must log:
   - git commit hash
   - config file used
   - seed
   - train/val/test split identifier
3) Splits must be **fixed and saved**.
4) Use blocked/time-aware splits if the data has time-like structure (avoid leakage).
5) Figures/tables must be generated by `analysis/run_all.py` from logged metrics.

---

## 9) How We Measure Success

Minimum viable “paper-ready” evidence:
- Strong PF baseline + ablations
- PF/OPF multitask results (or a clear explanation if negative transfer occurs)
- SSL pretraining with low-label gains and/or robustness gains
- Cascade transfer results + explanation fidelity evaluation
- Physics residual reported and improved by our methods
- Clean, reproducible scripts + documented configs

---

## 10) Collaboration Workflow

- Create a branch per feature/fix: `feature/<short-name>`
- Open a PR early (draft PRs encouraged)
- PR must include:
  - short summary
  - what changed
  - how to run
  - a screenshot / log snippet of key result (if training-related)

### Coding standards
- Prefer small, testable functions
- Type hints where helpful
- No silent exception swallowing
- Validate tensor shapes and device placement
- Write at least one basic test when adding a new metric or loader

---

## 11) Common Pitfalls (Avoid These)

- **Angle wrap errors**: don’t MSE raw angles.
- **Data leakage**: random splits can leak seasonal/time structure.
- **Edge order mismatch**: explanation masks must align with `edge_index`.
- **Unlogged configs**: untracked “magic runs” aren’t usable in a paper.

---

## 12) Quick Commands (Examples)

> These are placeholders—your actual config names may differ.

- Preprocess dataset:
  - `python scripts/preprocess_powergraph.py --raw data/raw --out data/processed`

- PF training:
  - `python scripts/train_pf.py --config configs/pf.yaml`

- PF+OPF multitask:
  - `python scripts/train_pf_opf_multitask.py --config configs/pf_opf.yaml`

- SSL pretrain:
  - `python scripts/pretrain_ssl.py --config configs/ssl.yaml`

- Fine-tune after SSL:
  - `python scripts/finetune_from_ssl.py --config configs/finetune_pf.yaml`

- Cascade training + explanation eval:
  - `python scripts/train_cascade.py --config configs/cascade.yaml`

- Regenerate figures/tables:
  - `python analysis/run_all.py --logs runs/ --out analysis_outputs/`

---

## 13) Contacts / Ownership

- Project lead: <Name / Email>
- Data owner: <Student A>
- PF + physics owner: <Student B>
- OPF multitask owner: <Student C>
- SSL owner: <Student D>
- Cascade + explanation owner: <Student E>
- Repro/DevOps owner (optional): <Student F>

---

## 14) What to Do If You’re Stuck

1) Run `scripts/smoke_test.py`
2) Run `scripts/inspect_dataset.py` and confirm basic stats
3) Reduce to a tiny debug config (32 graphs, 1 epoch)
4) Check:
   - shapes
   - NaNs/Infs
   - device mismatch (CPU/GPU)
   - split file correctness
5) Post:
   - command you ran
   - config file
   - stack trace
   - last 20 lines of logs
   in the team channel / issue tracker.

---

Welcome aboard — the goal is *rigorous, reproducible* science first, and performance second.
